# ðŸš€ DEPLOY & SCALE GUIDE
## Web Intelligence Platform - Production Deployment

---

## PHASE 1: LOCAL TESTING

### 1.1 Run Unit Tests

```bash
cd backend
pip install pytest pytest-asyncio

# Run tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=app
```

### 1.2 Run Load Tests

```bash
cd backend
pip install httpx

# Make sure docker-compose is running
docker-compose up --build

# In another terminal:
python load_test.py
```

**Load test output:**
```
ðŸ”¥ Starting Load Test
   Concurrent clients: 10
   Requests per client: 20
   Total requests: 200

ðŸ“Š Testing health endpoint...
   Health: âœ… (0.02s)

ðŸ“‹ Testing list sites...
   List: âœ… (0.05s)

ðŸŒ Load testing create sites...
   Completed 50 requests...
   Completed 100 requests...
   Completed 150 requests...
   Completed 200 requests...

âœ… Results:
   Total requests: 200
   Successful: 200
   Failed: 0
   Success rate: 100.0%
   Total time: 12.45s
   Throughput: 16.1 req/s

â±ï¸  Response times:
   Min: 0.045s
   Max: 0.123s
   Avg: 0.062s
   Median: 0.058s

ðŸ“ˆ Percentiles:
   p50: 0.058s
   p95: 0.105s
   p99: 0.120s
```

---

## PHASE 2: DOCKER DEPLOYMENT

### 2.1 Build Production Images

```bash
# Build with production tags
docker build -t your-registry/web-intelligence-backend:v1.0 ./backend
docker build -t your-registry/web-intelligence-frontend:v1.0 ./frontend

# Push to registry
docker push your-registry/web-intelligence-backend:v1.0
docker push your-registry/web-intelligence-frontend:v1.0
```

### 2.2 Docker Compose Production

Create `docker-compose.prod.yml`:

```yaml
version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: wip
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: web_intelligence
    volumes:
      - postgres_prod:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U wip"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    restart: unless-stopped

  backend:
    image: your-registry/web-intelligence-backend:v1.0
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql+asyncpg://wip:${DB_PASSWORD}@postgres:5432/web_intelligence
      REDIS_URL: redis://redis:6379/0
      DEBUG: "False"
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  frontend:
    image: your-registry/web-intelligence-frontend:v1.0
    ports:
      - "3000:3000"
    environment:
      REACT_APP_API_URL: https://api.yourdomain.com/api/v1
    restart: unless-stopped

volumes:
  postgres_prod:
```

**Deploy:**
```bash
# Set environment
export DB_PASSWORD="your-secure-password"

# Run
docker-compose -f docker-compose.prod.yml up -d

# Monitor
docker-compose -f docker-compose.prod.yml logs -f
```

---

## PHASE 3: KUBERNETES DEPLOYMENT (SCALING)

### 3.1 Prerequisites

```bash
# Install kubectl
brew install kubectl

# Install Helm (optional but recommended)
brew install helm

# Configure kubectl to connect to your cluster
kubectl config use-context your-cluster-name

# Verify connection
kubectl cluster-info
```

### 3.2 Create Namespace

```bash
kubectl apply -f k8s/namespace.yaml

# Verify
kubectl get namespace web-intelligence
```

### 3.3 Deploy Database

```bash
# Create secrets first
kubectl create secret generic postgres-secret \
  --from-literal=POSTGRES_PASSWORD=your-prod-password \
  -n web-intelligence

# Deploy
kubectl apply -f k8s/postgres-deployment.yaml

# Wait for ready
kubectl wait --for=condition=ready pod -l app=postgres \
  -n web-intelligence --timeout=300s

# Verify
kubectl get pods -n web-intelligence
```

### 3.4 Deploy Backend (with Autoscaling)

```bash
# Update image in backend-deployment.yaml
# Change: web-intelligence:backend-latest to your registry

# Deploy
kubectl apply -f k8s/backend-deployment.yaml

# Check deployment
kubectl get deployment -n web-intelligence
kubectl get hpa -n web-intelligence

# Monitor
kubectl logs -f deployment/backend -n web-intelligence
```

### 3.5 Deploy Frontend (with Autoscaling)

```bash
# Update image and API URL in frontend-deployment.yaml

# Deploy
kubectl apply -f k8s/frontend-deployment.yaml

# Check
kubectl get deployment -n web-intelligence
```

### 3.6 Deploy Monitoring (Prometheus + Grafana)

```bash
kubectl apply -f k8s/monitoring-stack.yaml

# Wait for pods
kubectl wait --for=condition=ready pod -l app=prometheus \
  -n web-intelligence --timeout=300s

# Port forward for access
kubectl port-forward -n web-intelligence \
  svc/prometheus 9090:9090

kubectl port-forward -n web-intelligence \
  svc/grafana 3001:3001

# Access:
# - Prometheus: http://localhost:9090
# - Grafana: http://localhost:3001 (admin/admin)
```

### 3.7 Create Ingress (External Access)

Create `k8s/ingress.yaml`:

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-intelligence
  namespace: web-intelligence
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - api.yourdomain.com
    - app.yourdomain.com
    secretName: tls-secret
  rules:
  - host: api.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: backend
            port:
              number: 8000
  - host: app.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend
            port:
              number: 3000
```

```bash
kubectl apply -f k8s/ingress.yaml
```

---

## PHASE 4: AUTOSCALING CONFIGURATION

### 4.1 Horizontal Pod Autoscaling (Already in deployment YAML)

**Backend scales:**
- Min: 3 pods
- Max: 10 pods
- Trigger: CPU 70% or Memory 80%

**Frontend scales:**
- Min: 3 pods
- Max: 8 pods
- Trigger: CPU 75% or Memory 80%

### 4.2 Monitor Scaling

```bash
# Watch HPA
kubectl get hpa -n web-intelligence -w

# Detailed HPA info
kubectl describe hpa backend-hpa -n web-intelligence
```

### 4.3 Manual Scaling (if needed)

```bash
# Scale backend to 5 replicas
kubectl scale deployment backend --replicas=5 -n web-intelligence

# Scale frontend to 4 replicas
kubectl scale deployment frontend --replicas=4 -n web-intelligence
```

---

## PHASE 5: PERFORMANCE TUNING

### 5.1 Database Optimization

```bash
# Connect to PostgreSQL
kubectl port-forward postgres-0 5432:5432 -n web-intelligence

# In another terminal:
psql -h localhost -U wip -d web_intelligence

# Run optimization
VACUUM ANALYZE;
REINDEX;
```

### 5.2 Backend Tuning

In `backend-deployment.yaml`:
```yaml
# Adjust worker settings
- name: backend
  env:
  - name: WORKERS
    value: "4"
  - name: TIMEOUT
    value: "120"
```

### 5.3 Connection Pooling

```yaml
# In database configuration
DATABASE_POOL_SIZE: "20"
DATABASE_MAX_OVERFLOW: "40"
```

---

## PHASE 6: MONITORING & ALERTS

### 6.1 Key Metrics to Monitor

**Backend:**
- Request latency (p50, p95, p99)
- Error rate
- Throughput (requests/sec)
- Pod CPU/Memory usage
- Database connection pool

**Frontend:**
- Page load time
- Error rate
- Pod CPU/Memory usage

**Infrastructure:**
- Pod count
- Node CPU/Memory
- Network I/O
- Storage usage

### 6.2 Grafana Dashboards

Create dashboard with panels:

1. **Request Latency**
   ```
   histogram_quantile(0.95, http_request_duration_seconds_bucket)
   ```

2. **Error Rate**
   ```
   rate(http_requests_total{status=~"5.."}[5m])
   ```

3. **Throughput**
   ```
   rate(http_requests_total[5m])
   ```

4. **Pod CPU**
   ```
   container_cpu_usage_seconds_total
   ```

5. **Pod Memory**
   ```
   container_memory_usage_bytes
   ```

### 6.3 Alert Rules

Already configured in `k8s/monitoring-stack.yaml`:
- High error rate (>5%)
- High latency (p95 > 1s)
- Pod down
- Memory threshold (>80%)
- CPU threshold (>90%)

---

## PHASE 7: LOAD TESTING AT SCALE

### 7.1 Simulate 1000 Concurrent Users

```bash
# Create advanced load test
python3 << 'EOF'
import asyncio
import httpx
import random
from concurrent.futures import ThreadPoolExecutor

async def simulate_users(num_users=1000, duration_seconds=300):
    """Simulate 1000 concurrent users"""
    
    print(f"Simulating {num_users} concurrent users for {duration_seconds}s")
    
    start_time = time.time()
    endpoints = [
        "/api/v1/sites",
        "/api/v1/sites?limit=50",
        "/api/v1/jobs",
        "/api/v1/analytics/dashboard",
    ]
    
    async def user_session():
        async with httpx.AsyncClient() as client:
            while time.time() - start_time < duration_seconds:
                endpoint = random.choice(endpoints)
                try:
                    response = await client.get(
                        f"http://api.yourdomain.com{endpoint}",
                        timeout=10
                    )
                except:
                    pass
                await asyncio.sleep(random.uniform(1, 5))
    
    tasks = [user_session() for _ in range(num_users)]
    await asyncio.gather(*tasks)

asyncio.run(simulate_users(1000, 300))
EOF
```

### 7.2 Monitor During Load

```bash
# In one terminal: watch metrics
watch -n 1 'kubectl top pods -n web-intelligence'

# In another: watch scaling
watch -n 1 'kubectl get hpa -n web-intelligence'

# In another: check pods
watch -n 1 'kubectl get pods -n web-intelligence'
```

---

## PHASE 8: TROUBLESHOOTING

### 8.1 Pod Crash Loop

```bash
# Check pod status
kubectl describe pod <pod-name> -n web-intelligence

# Check logs
kubectl logs <pod-name> -n web-intelligence --previous

# Check events
kubectl get events -n web-intelligence
```

### 8.2 Database Connection Errors

```bash
# Check database connectivity
kubectl exec -it deployment/backend -n web-intelligence \
  -- curl http://postgres:5432

# Check database logs
kubectl logs postgres-0 -n web-intelligence
```

### 8.3 High Memory Usage

```bash
# Check memory by pod
kubectl top pods -n web-intelligence --sort-by=memory

# Adjust limits in deployment YAML
# Increase limits or reduce replicas
```

---

## PHASE 9: BACKUP & DISASTER RECOVERY

### 9.1 Backup Database

```bash
# Daily backup cronjob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: web-intelligence
spec:
  schedule: "0 2 * * *"  # 2 AM daily
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:15-alpine
            command:
            - /bin/sh
            - -c
            - pg_dump -h postgres -U wip web_intelligence | gzip > /backup/db-$(date +%Y%m%d-%H%M%S).sql.gz
            volumeMounts:
            - name: backup
              mountPath: /backup
          volumes:
          - name: backup
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
```

### 9.2 Restore from Backup

```bash
# Get backup file
kubectl cp web-intelligence/postgres-backup:backup/db-*.sql.gz .

# Restore
kubectl exec -i postgres-0 -n web-intelligence \
  -- gunzip | psql -U wip web_intelligence
```

---

## PHASE 10: PRODUCTION CHECKLIST

### Pre-Production
- [x] Code tested locally
- [x] Load tests passed
- [x] Docker images built
- [x] Kubernetes manifests created
- [x] Monitoring configured
- [ ] SSL certificates issued
- [ ] Domain configured
- [ ] Backup strategy tested
- [ ] Disaster recovery tested
- [ ] Team trained

### Deployment Day
- [ ] Database migrated
- [ ] Kubernetes cluster healthy
- [ ] Services deployed
- [ ] Monitoring active
- [ ] Alerts configured
- [ ] Health checks passing
- [ ] Smoke tests passed
- [ ] Team on standby

### Post-Deployment
- [ ] Monitor metrics
- [ ] Watch error logs
- [ ] Verify autoscaling works
- [ ] Performance baseline
- [ ] Document issues
- [ ] Plan improvements

---

## SCALING EXPECTATIONS

### Single Container (Docker)
- **Throughput**: ~16 req/s
- **Latency**: p95 < 100ms
- **Max users**: 100-200 concurrent

### 3 Pods (Kubernetes)
- **Throughput**: ~48 req/s
- **Latency**: p95 < 50ms
- **Max users**: 300-600 concurrent

### 10 Pods (Full Scale)
- **Throughput**: ~160 req/s
- **Latency**: p95 < 30ms
- **Max users**: 1000-2000 concurrent

### With Load Balancing
- **Throughput**: Scales linearly
- **Latency**: Consistent
- **Max users**: Virtually unlimited

---

## COST ESTIMATION

### Development (Docker)
- Server: 1 small instance ($20/month)
- Storage: 10GB ($1/month)
- **Total: ~$21/month**

### Staging (Kubernetes)
- Nodes: 3 small instances ($60/month)
- Storage: 20GB ($2/month)
- **Total: ~$62/month**

### Production (Kubernetes)
- Nodes: 5-10 medium instances ($100-200/month)
- Storage: 100GB ($5/month)
- Backup: 50GB ($5/month)
- **Total: ~$110-210/month**

---

## SUCCESS METRICS

| Metric | Target | Actual |
|--------|--------|--------|
| Availability | 99.9% | âœ… TBD |
| p95 Latency | < 100ms | âœ… TBD |
| Error Rate | < 0.1% | âœ… TBD |
| Throughput | 100+ req/s | âœ… TBD |
| Max Concurrent | 1000+ users | âœ… TBD |
| Recovery Time | < 30s | âœ… TBD |

---

## NEXT STEPS

1. **Complete Phase 1-2** - Local testing & Docker
2. **Complete Phase 3-4** - Kubernetes & autoscaling  
3. **Complete Phase 5-6** - Performance tuning & monitoring
4. **Complete Phase 7-9** - Load testing & backups
5. **Phase 10** - Go live!

---

**You're now ready to deploy and scale to thousands of users.** ðŸš€


